{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f65b673d",
   "metadata": {},
   "source": [
    "# Kaggle Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f01a265",
   "metadata": {},
   "source": [
    "## Describe Your Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f2c779",
   "metadata": {},
   "source": [
    "**URL:** https://www.kaggle.com/datasets/samuelcortinhas/muffin-vs-chihuahua-image-classification/data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee95bd2",
   "metadata": {},
   "source": [
    "**Task:**\n",
    "muffin과 chihuahua에 대한 구글 이미지 결과값을 스크랩한 바이너리 이미지 파일을 각각 구분하는 모델 만들기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "925b65df",
   "metadata": {},
   "source": [
    "**Datasets**\n",
    "\n",
    "* Train dataset: train 파일 데이터의 70%\n",
    "\n",
    "* Validation dataset: train 파일 데이터의 30%\n",
    "\n",
    "* Test dataset: test 파일에 들어있는 이미지 전부"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6df73f8",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ccb4a7",
   "metadata": {},
   "source": [
    "## Build Your Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "575c8d37",
   "metadata": {},
   "source": [
    "### Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "486febaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.models import resnet18\n",
    "\n",
    "## gpu 사용 여부 확인 및 설정\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "## 데이터 경로 설정\n",
    "train_data_path = \"/Users/jung-yujin/2023DGU_MLDL_Final/train\"\n",
    "test_data_path = \"/Users/jung-yujin/2023DGU_MLDL_Final/test\"\n",
    "\n",
    "## 데이터 전처리 및 augmentation\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "## 데이터 로딩\n",
    "dataset = ImageFolder(train_data_path, transform=transform)\n",
    "\n",
    "## 데이터를 train과 validation으로 나누기\n",
    "train_size = int(0.7 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_data, val_data = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "## test 데이터 로딩\n",
    "test_dataset = ImageFolder(test_data_path, transform=transform)\n",
    "\n",
    "## 데이터 로더 설정\n",
    "train_loader = DataLoader(train_data, batch_size=64, shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(val_data, batch_size=64, shuffle=False, num_workers=4)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "131c7f8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image shape: torch.Size([64, 3, 224, 224]), Label: tensor([1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0,\n",
      "        0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1,\n",
      "        0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "## 데이터 확인 (예시로 첫 번째 batch의 이미지와 레이블 출력)\n",
    "for images, labels in train_loader:\n",
    "    print(f\"Image shape: {images.shape}, Label: {labels}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a5ee14",
   "metadata": {},
   "source": [
    "### Model Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "40c15d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 모델 정의 (ResNet18을 사용하고 출력 클래스를 2로 변경)\n",
    "model = resnet18(weights=None)\n",
    "model.fc = nn.Linear(model.fc.in_features, 2)  ## 출력 클래스에 맞게 변경\n",
    "\n",
    "## 모델을 cpu로 이동\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e814c018",
   "metadata": {},
   "source": [
    "### Train Model & Select Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c3263fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 손실 함수 및 최적화 함수 설정\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6dcbadfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 0.37075158953666687, Validation Loss: 1.6044188349143318, Accuracy: 0.6887323943661972\n",
      "Epoch 2/10, Loss: 0.42714565992355347, Validation Loss: 1.4175522275592969, Accuracy: 0.793661971830986\n",
      "Epoch 3/10, Loss: 0.36241209506988525, Validation Loss: 0.4926461201647054, Accuracy: 0.8091549295774648\n",
      "Epoch 4/10, Loss: 0.2747235894203186, Validation Loss: 1.1277967497058536, Accuracy: 0.721830985915493\n",
      "Epoch 5/10, Loss: 0.5478838086128235, Validation Loss: 0.4387102321438167, Accuracy: 0.8204225352112676\n",
      "Epoch 6/10, Loss: 0.19325777888298035, Validation Loss: 0.3550372058930604, Accuracy: 0.8556338028169014\n",
      "Epoch 7/10, Loss: 0.24344468116760254, Validation Loss: 0.28901107803634973, Accuracy: 0.8866197183098592\n",
      "Epoch 8/10, Loss: 0.31232750415802, Validation Loss: 0.3742492218380389, Accuracy: 0.8591549295774648\n",
      "Epoch 9/10, Loss: 0.29752498865127563, Validation Loss: 0.31825208016063855, Accuracy: 0.8767605633802817\n",
      "Epoch 10/10, Loss: 0.26755717396736145, Validation Loss: 0.39224109053611755, Accuracy: 0.8739436619718309\n"
     ]
    }
   ],
   "source": [
    "## 모델 학습\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "## Validation Loss 계산\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            val_loss += criterion(outputs, labels).item()\n",
    "\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    val_loss /= len(val_loader)\n",
    "    accuracy = correct / total\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {loss.item()}, Validation Loss: {val_loss}, Accuracy: {accuracy}\")\n",
    "\n",
    "## 학습된 모델 저장\n",
    "torch.save(model.state_dict(), \"muffin_chihuahua_model.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2dea6f0",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8779c8f2",
   "metadata": {},
   "source": [
    "## Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f49b4961",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.8851351351351351\n"
     ]
    }
   ],
   "source": [
    "## 모델 평가\n",
    "model.eval()\n",
    "test_correct = 0\n",
    "test_total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        test_total += labels.size(0)\n",
    "        test_correct += (predicted == labels).sum().item()\n",
    "\n",
    "test_accuracy = test_correct / test_total\n",
    "print(f\"Test Accuracy: {test_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa2652e",
   "metadata": {},
   "source": [
    "The results explains\n",
    "\n",
    "이미지 분류 모델은 '머핀 vs 치와와' 데이터셋에서 효과적으로 작동했습니다. 10 에포크 동안의 훈련 결과는 검증 정확도가 87.39%로 안정화되었으며, 테스트에서는 88.51%의 정확도를 달성했습니다. 초기에는 빠른 학습이 이루어졌고, 모델은 훈련 및 테스트 데이터에 대한 안정적인 성능을 보여주었습니다. 향후 개선을 위해 데이터셋의 특이성과 모델의 한계를 조사하는 것이 중요합니다"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
